---
title: "SR 15"
output: html_document
date: "2023-02-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### **15E1. Rewrite the Oceanic tools model (from Chapter 11) below so that it assumes measured error  on the log population sizes of each society. You don’t need to fit the model to data. Just modify the  mathematical formula below.**\ 
$$
T_i \sim Poisson(\mu_i) \\
log \ \mu_i = \alpha + \beta \ logP_i\\
\alpha \sim Normal(0, 1.5)\\
\beta \sim Normal(0, 1)  
$$
Rewritten:\
$$
T_i \sim Poisson(\mu_i) \\
log \ \mu_i = \alpha + \beta \ logP_{TRUE,i} \\
P_{OBS,i} \sim Normal(P_{TRUE,i},P_{SE,i})\\
P_{TRUE,i} \sim Normal(0,1)\\
\alpha \sim Normal(0, 1.5)\\
\beta \sim Normal(0, 1)  
$$


### **15E2. Rewrite the same model so that it allows imputation of missing values for log population.  There aren’t any missing values in the variable, but you can still write down a model formula that would imply imputation, if any values were missing.**\

Q: in the model do i work with just P_i or log P_i?

$$
T_i \sim Poisson(\mu_i) \\
log \ \mu_i = \alpha + \beta \ logP_i \\
log\ P_i \sim Normal(\nu,\sigma_P)\\
\alpha \sim Normal(0, 1.5)\\
\beta \sim Normal(0, 1)\\ 
\nu \sim Normal(0.5,1)\\
\sigma_P \sim Exponential(1)
$$

### **15M1. Using the mathematical form of the imputation model in the chapter, explain what is being assumed about how the missing values were generated. ** 

In the chapter, the missing values are assumed to not contain any information abou the individual cases. The missing values instead are randomly placed across the cases. The model assumes that there is no causation related to missingness. Instead, the model just states that missing data follows a certain distribution which we would use to test against the data we have presently. \

### **15M2. Reconsider the primate milk missing data example from the chapter. This time, assign B a  distribution that is properly bounded between zero and 1. A beta distribution, for example, is a good choice.  **

[need to work on getting this code to work]
```{r}
library(rethinking)
library(tidyverse)
data(milk)
d <- tibble(milk)
data3 <- d %>% 
  mutate(neocortex_prop = neocortex.perc/100,
         logmass = log(mass),
         K = standardize(kcal.per.g),
         B = standardize(neocortex_prop),
         M = standardize(logmass)) %>% 
  select(K,B,M)


#m15.5  <- ulam(
#alist(
#K ~ dnorm(mu,sigma),
#mu <- a + bB * B + bM * M,
#B ~ dbeta(alpha, beta), # change to beta distribution
#alpha ~ dnorm (0,1),
#beta ~ dnorm(0,1),
#a ~ dnorm(0,0.5),
#bB ~ dnorm(0,0.5),
#bM ~ dnorm(0,0.5),
#sigma ~ dexp(1)
#) , data = data3, chains = 4, cores = 4)

```


### **15M3. Repeat the divorce data measurement error models, but this time double the standard errors.  Can you explain how doubling the standard errors impacts inference?**\
```{r}

data(WaffleDivorce)
x <-WaffleDivorce
dlist <-list(
D_obs = standardize(x$Divorce),
D_sd_2 = 2*(x$Divorce.SE/sd(x$Divorce)),
M = standardize(x$Marriage),
A = standardize(x$MedianAgeMarriage),
N = nrow(x))

set.seed(7734)
m15.1 <-ulam(
alist(
D_obs ~ dnorm(D_true, D_sd_2),
vector[N]:D_true ~ dnorm(mu,sigma),
mu <-a+bA*A+bM*M,
a ~ dnorm(0,0.2),
bA ~ dnorm(0,0.5),
bM ~ dnorm(0,0.5),
sigma ~ dexp(1)
) ,data = dlist, chains = 4, cores = 4)

precis(m15.1, 2)
```
Doubling `D_sd` makes the error term in the model larger, indicating that the model has more uncertainty in its predictions. We can see evidence of this from the `n_eff` and `rhat4` values. The n_Eff values are mostly under 100 and the rhat values are over 1. This indicates that the chains have not convereged and the parameters are not well estimated and may not be reliable. This makes sense because we have less certainty about the model now that we doubled the error. In conclusions, doubling the standard errors makes the model less confident in its predictions, which could lead to less accurate inferences about the relationship between the variables.\

### **15M4. Simulate data from this DAG: X → Y → Z. Now fit a model that predicts Y using both X  and Z. What kind of confound arises, in terms of inferring the causal influence of X on Y?  **

```{r}
# simulate the variables

set.seed(123)

n <- 1000

X <- rnorm(n)
Y <- 0.5 * X + rnorm(n)
Z <- 0.3 * Y + rnorm(n)

data <- tibble(X, Y, Z)


# create the model

m4 <- ulam(
  alist(
    Y ~ dnorm(mu, sigma),
    mu <- a + b_X * X + b_Z * Z,
    a ~ dnorm(0, 1),
    b_X ~ dnorm(0, 1),
    b_Z ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = data
)

precis(m4)
```

A confound arises when inferring the causal influence of X on Y because Z is a common cause of both X and Y. In other words, Z affects both the independent variable (X) and the dependent variable (Y). This makes  it difficult to determine the causal effect of X on Y solely from the data.\

### **15M5. Return to the singing bird model, m15.9, and compare the posterior estimates of cat presence  (PrC1) to the true simulated values. How good is the model at inferring the missing data? Can you  think of a way to change the simulation so that the precision of the inference is stronger?  **

```{r}
# data
set.seed(9) 
N_houses <-2000L
alpha <-5
beta <-(-3)
k <-0.5
r <-0.2
cat <-rbern(N_houses,k)
notes <-rpois(N_houses,alpha + beta * cat)
R_C <-rbern(N_houses,r)
cat_obs <-cat
cat_obs[R_C==1] <- (-9L)

dat <- list(
notes = notes,
cat = cat_obs,
RC = R_C,
N = as.integer(N_houses))

# model
m15.9 <- ulam(
alist(
# singing bird model
notes|RC==0 ~ poisson(lambda),
notes|RC==1 ~ custom(log_sum_exp(
  log(k) + poisson_lpmf(notes | exp(a+b)),
  log(1-k) + poisson_lpmf(notes | exp(a))
) ),
log(lambda) <- a + b * cat,
a ~ normal(0,1),
b ~ normal(0,0.5),
# sneaking cat model
cat | RC==0 ~ bernoulli(k),
k ~ beta(2,2),
# imputed values
gq > vector[N]:PrC1 <- exp(lpC1) / (exp(lpC1) + exp(lpC0)),
gq > vector[N]:lpC1 <- log(k) + poisson_lpmf(notes[i] | exp(a + b)),
gq > vector[N]:lpC0 <- log(1 - k) + poisson_lpmf(notes[i]|exp(a))
), data = dat, chains = 4, cores = 4)


# compare values

# using correlation to evaluate inference
post_samples <- extract.samples(m15.9)
cat_presence_estimates <- post_samples$PrC1
# mean_cat_presence_estimates <- rowMeans(post_samples$PrC1)
true_values <- as.numeric(dat$cat)
cor_result <- cor(cat_presence_estimates, true_values)
#print(cor_result)
summary(cor_result)
```
Skimming the results, we see that the values are not close to one and thus, the model is actually not doing a great job at inferring the missing data. One way to change the simulation to increase the precision of the inference is to increase the sample size, making the estimation of the parameters more precise. Another way is to collect more and/or better information to improve the model. For example, if we knew the number of notes produced by each bird, that could be used to further improve the inference.

### **15M6. Return to the four dog-eats-homework missing data examples. Simulate each and then fit  one or more models to try to recover valid estimates for S → H.  **

[need to work on second part of question later. ]
```{r}
# simulate our variables
N <- 100
S <- rnorm(N)
H <- rbinom(N, size = 10, inv_logit(S))

# simulate the random eating example
D <-rbern(N) # dogs completely random
Hm <- H
Hm[D==1] <- NA

#simulate that S -> D where studing influences dog eating hw
D2  <- ifelse(S > 0, 1, 0)
Hm2 <- H
Hm2[D2==1] <- NA


# now for DAG c
set.seed(501) 
N3 <- 1000
X3 <- rnorm(N3)
S3 <- rnorm(N3)
H3 <- rbinom(N3, size = 10, inv_logit(2 + S3 - 2 * X3))

D3 <- ifelse(X3 > 1,1,0)
Hm3 <- H3
Hm3[D3==1] <- NA

# now for DAF d
N4 <- 100
S4 <- rnorm(N4)
H4 <- rbinom(N4, size = 10, inv_logit(S4))
D4 <- ifelse(H4 < 5,1,0)
Hm4 <- H4; Hm4[D4==1] <- NA
```

